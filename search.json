[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Large Soil Spectral Models (LSSM)",
    "section": "",
    "text": "This is a Python package allowing to reproduce the research work done by Franck Albinet in the context of a PhD @ KU Leuven titled “Multiscale Characterization of Exchangeable Potassium Content in Soil to Remediate Agricultural Land Affected by Radioactive Contamination using Machine Learning, Soil Spectroscopy and Remote Sensing”.\nOur first paper Albinet, F., Peng, Y., Eguchi, T., Smolders, E., Dercon, G., 2022. Prediction of exchangeable potassium in soil through mid-infrared spectroscopy and deep learning: From prediction to explainability. Artificial Intelligence in Agriculture 6, 230–241. investigated the possibility to predict exchangeable potassium in soil using large Mid-infrared soil spectral libraries and Deep Learning. Code available here.\nWe are now exploring the potential to characterize and predict exchangeable potassium using both Near- and Mid-infrared soil spectroscopy, with a focus on leveraging advanced Deep Learning models such as ResNet and ViT transformers through transfer learning.\nOur Deep Learning pipeline is primarily based on the approach described by Jeremy Howard."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Large Soil Spectral Models (LSSM)",
    "section": "Install",
    "text": "Install\npip install lssm"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Large Soil Spectral Models (LSSM)",
    "section": "Getting started",
    "text": "Getting started\nWe demonstrate a typical workflow below to showcase our method.\n\nfrom pathlib import Path\nfrom functools import partial\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\nfrom torch import optim, nn\n\nimport timm\n\nfrom torcheval.metrics import R2Score\nfrom torch.optim import lr_scheduler\nfrom lssm.loading import load_ossl\nfrom lssm.learner import Learner\nfrom lssm.preprocessing import ToAbsorbance, ContinuumRemoval, Log1p\nfrom lssm.dataloaders import SpectralDataset, get_dls\nfrom lssm.callbacks import (MetricsCB, BatchSchedCB, BatchTransformCB,\n                            DeviceCB, TrainCB, ProgressCB)\nfrom lssm.transforms import GADFTfm, _resizeTfm, StatsTfm\n\n\nLoading training & validation data\n\nLoad model from timm python package, Deep Learning State-Of-The-Art (SOTA) pre-trained models:\n\n\nmodel_name = 'resnet18'\nmodel = timm.create_model(model_name, pretrained=True, in_chans=1, num_classes=1)\n\n\nAutomatically download large spectral libraries developed by our colleagues at WCRC. We focus on exchangeable potassium in the example below:\n\n\nanalytes = 'k.ext_usda.a725_cmolc.kg'\ndata = load_ossl(analytes, spectra_type='visnir')\nX, y, X_names, smp_idx, ds_name, ds_label = data\n\nReading & selecting data ...\n\n\n\nA bit of data features and target preprocessing:\n\n\nX = Pipeline([('to_abs', ToAbsorbance()), \n              ('cr', ContinuumRemoval(X_names))]).fit_transform(X)\n\ny = Log1p().fit_transform(y)\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44489/44489 [00:15&lt;00:00, 2850.84it/s]\n\n\n\nTypical train/test split to get a train and valid dataset:\n\n\nn_smp = 5000 # For demo. purpose (in reality we have &gt; 50K)\nX_train, X_valid, y_train, y_valid = train_test_split(X[:n_smp, :], y[:n_smp], \n                                                      test_size=0.1,\n                                                      stratify=ds_name[:n_smp], \n                                                      random_state=41)\n\n\nFinally, creating a custom PyTorch DataLoader:\n\n\ntrain_ds, valid_ds = [SpectralDataset(X, y, ) \n                      for X, y, in [(X_train, y_train), (X_valid, y_valid)]]\n\n# Then PyTorch dataloaders\ndls = get_dls(train_ds, valid_ds, bs=32)\n\n\n\nTraining\n\nepochs = 1\nlr = 5e-3\n\n# We use `r2` along to assess performance\nmetrics = MetricsCB(r2=R2Score())\n\n# We use Once Cycle Learning Rate scheduling approach\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n\n# A series of preprocessing performed on GPUs\n#    - put to GPU\n#    - transform to 1D to 2D spectra using Gramian Angular Difference Field (GADF)\n#    - resize the 2D version\n#    - apply pre-trained model stats\nxtra = [BatchSchedCB(sched)]\ngadf = BatchTransformCB(GADFTfm())\nresize = BatchTransformCB(_resizeTfm)\nstats = BatchTransformCB(StatsTfm(model.default_cfg))\n\ncbs = [DeviceCB(), gadf, resize, stats, TrainCB(), \n       metrics, ProgressCB(plot=False)]\n\nlearn = Learner(model, dls, nn.MSELoss(), lr=lr, \n                cbs=cbs+xtra, opt_func=optim.AdamW)\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nr2\nloss\nepoch\ntrain\n\n\n\n\n0.348\n0.097\n0\ntrain\n\n\n0.545\n0.067\n0\neval"
  },
  {
    "objectID": "preprocessing.html",
    "href": "preprocessing.html",
    "title": "Preprocessing",
    "section": "",
    "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nfrom lssm.loading import load_ossl, load_mir_kex_spike, load_nir_kex_spike\nfrom lssm.visualization import plot_spectra\nfrom sklearn.pipeline import Pipeline\nsource",
    "crumbs": [
      "API",
      "Preprocessing"
    ]
  },
  {
    "objectID": "preprocessing.html#scikit-learn-transformers",
    "href": "preprocessing.html#scikit-learn-transformers",
    "title": "Preprocessing",
    "section": "Scikit-learn transformers",
    "text": "Scikit-learn transformers\n\nsource\n\nToAbsorbance\n\n ToAbsorbance (eps=1e-05)\n\nTransform Reflectance to Absorbance\nExample:\n\nanalytes = 'k.ext_usda.a725_cmolc.kg'\ndata = load_ossl(analytes, spectra_type='visnir')\nX, y, X_names, smp_idx, ds_name, ds_label = data\n\nReading & selecting data ...\n\n\n\nplot_spectra(ToAbsorbance().fit_transform(X), X_names)\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nContinuumRemoval\n\n ContinuumRemoval (wls)\n\nCreates continnum removal custom transformer\nExample:\n\npipe = Pipeline([('to_abs', ToAbsorbance()), \n                 ('cr', ContinuumRemoval(X_names))])\nplot_spectra(pipe.fit_transform(X), X_names)\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44489/44489 [00:15&lt;00:00, 2808.72it/s]\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nSNV\n\n SNV ()\n\nCreates scikit-learn SNV custom transformer\n\npipe = Pipeline([('to_abs', ToAbsorbance()),\n                 ('cr', ContinuumRemoval(X_names)),\n                 ('snv', SNV())])\nplot_spectra(pipe.fit_transform(X), X_names)\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44489/44489 [00:15&lt;00:00, 2850.45it/s]\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nLog1p\n\n Log1p ()\n\nCreates scikit-learn np.log1p target custom transformer\n\nsource\n\n\nSpikeDiff\n\n SpikeDiff (names)\n\n*Scikit-learn transformer for taking the difference of spiked sample spectra.\nAttributes: names (list): List of names to be processed. idx (numpy.ndarray): Indices for differences in spectra.*\n\nsource\n\n\nTakeDerivative\n\n TakeDerivative (window_length=11, polyorder=1, deriv=1)\n\n*Creates scikit-learn derivation custom transformer\nArgs: window_length: int, optional Specify savgol filter smoothing window length\npolyorder: int, optional\n    Specify order of the polynom used to interpolate derived signal\n\nderiv: int, optional\n    Specify derivation degree\nReturns: scikit-learn custom transformer*\n\nsrc_dir = Path().home() / 'pro/data/k-spiking/mir'\nX, wavenumbers, names = load_mir_kex_spike(src_dir)\n\n\nplot_spectra(TakeDerivative().fit_transform(X), wavenumbers)\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nMinScaler\n\n MinScaler ()\n\n*Base class for all estimators in scikit-learn.\nInheriting from this class provides default implementations of:\n\nsetting and getting parameters used by GridSearchCV and friends;\ntextual and HTML representation displayed in terminals and IDEs;\nestimator serialization;\nparameters validation;\ndata validation;\nfeature names validation.\n\nRead more in the :ref:User Guide &lt;rolling_your_own_estimator&gt;.*\n\nplot_spectra(MinScaler().fit_transform(X), wavenumbers, ascending=False)\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nMeanCenter\n\n MeanCenter ()\n\n*Base class for all estimators in scikit-learn.\nInheriting from this class provides default implementations of:\n\nsetting and getting parameters used by GridSearchCV and friends;\ntextual and HTML representation displayed in terminals and IDEs;\nestimator serialization;\nparameters validation;\ndata validation;\nfeature names validation.\n\nRead more in the :ref:User Guide &lt;rolling_your_own_estimator&gt;.*\n\nplot_spectra(MeanCenter().fit_transform(X), wavenumbers, ascending=False)\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nplot_spectra(X[1], wavenumbers, ascending=False)\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nBaselineALS\n\n BaselineALS (lam=100000.0, p=0.01, niter=10)\n\n*Base class for all estimators in scikit-learn.\nInheriting from this class provides default implementations of:\n\nsetting and getting parameters used by GridSearchCV and friends;\ntextual and HTML representation displayed in terminals and IDEs;\nestimator serialization;\nparameters validation;\ndata validation;\nfeature names validation.\n\nRead more in the :ref:User Guide &lt;rolling_your_own_estimator&gt;.*\n\nplot_spectra(X, wavenumbers, ascending=False)\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nplot_spectra(BaselineALS().fit_transform(X), wavenumbers, ascending=False)\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [00:00&lt;00:00, 82.47it/s]\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nInterpolate\n\n Interpolate (old_indexes:numpy.ndarray, new_indexes:numpy.ndarray)\n\nInterpolate data according to new indices\n\n\n\n\nType\nDetails\n\n\n\n\nold_indexes\nndarray\nOld wavenumbers or wavelength\n\n\nnew_indexes\nndarray\nNew wavenumbers or wavelength\n\n\n\n\nsource\n\n\nSpikeMean\n\n SpikeMean (names)\n\n*Base class for all estimators in scikit-learn.\nInheriting from this class provides default implementations of:\n\nsetting and getting parameters used by GridSearchCV and friends;\ntextual and HTML representation displayed in terminals and IDEs;\nestimator serialization;\nparameters validation;\ndata validation;\nfeature names validation.\n\nRead more in the :ref:User Guide &lt;rolling_your_own_estimator&gt;.*\n\nscan_type = 'MIR'\n\nif scan_type == 'MIR':\n    src_dir = Path().home() / 'pro/data/k-spiking/mir'\n    X, wavenumbers, names = load_mir_kex_spike(src_dir)\nelse:\n    fname = Path().home() / 'pro/data/k-spiking/nir/2023-12-8 _FT-NIR-K-spiked soil.xlsx'\n    X, wavenumbers, names = load_nir_kex_spike(fname)\n\n\nX_mean, names_mean = SpikeMean(names).fit_transform(X)\n\nprint(f'X shape: {X_mean.shape}')\nprint(f'Sample names: {names_mean}')\n\nX shape: (12, 1738)\nSample names: ['LUI-0' 'LUI-1' 'LUI-2' 'LUI-3' 'SPA1-0' 'SPA1-1' 'SPA1-2' 'SPA1-3'\n 'TM4.1-0' 'TM4.1-1' 'TM4.1-2' 'TM4.1-3']\n\n\n\n\nDWT\n\n# class SpikeDWT(BaseEstimator, TransformerMixin):\n#     def __init__(self, \n#                  names, \n#                  wavenumbers,\n#                  top_percentile=1,\n#                  levels=range(4, 9),\n#                  wavelet_name='db1'):\n#         fc.store_attr()\n#         self.data = {'level': [], 'smp_name': [],\n#                      'wavenumber': [], 'coeff': [],\n#                      'resolution': [] # in wavenumbers\n#                      }\n        \n#     X_mean, names_mean",
    "crumbs": [
      "API",
      "Preprocessing"
    ]
  },
  {
    "objectID": "paper/visnir-kex.html",
    "href": "paper/visnir-kex.html",
    "title": "Visnir LSSM for Kex",
    "section": "",
    "text": "# !pip install --upgrade lssm\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nimport lssm\nlssm.__version__\n\n'0.1.1'",
    "crumbs": [
      "Paper with code",
      "Visnir LSSM for Kex"
    ]
  },
  {
    "objectID": "paper/visnir-kex.html#imports",
    "href": "paper/visnir-kex.html#imports",
    "title": "Visnir LSSM for Kex",
    "section": "Imports",
    "text": "Imports\n\nfrom pathlib import Path\nfrom functools import partial\nimport fastcore.all as fc\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\n\nimport timm\nimport torch\nfrom torcheval.metrics import R2Score\nfrom torch.optim import lr_scheduler\nfrom torch import optim, nn\n\nfrom lssm.loading import load_ossl\nfrom lssm.preprocessing import (ToAbsorbance, ContinuumRemoval, MeanCenter, \n                                Log1p, SNV, Interpolate, wl_to_wn)\nfrom lssm.dataloaders import SpectralDataset, get_dls\nfrom lssm.callbacks import (MetricsCB, BatchSchedCB, BatchTransformCB,\n                            DeviceCB, TrainCB, ProgressCB)\nfrom lssm.transforms import GADFTfm, _resizeTfm, StatsTfm\nfrom lssm.learner import Learner\nfrom lssm.visualization import plot_spectra",
    "crumbs": [
      "Paper with code",
      "Visnir LSSM for Kex"
    ]
  },
  {
    "objectID": "paper/visnir-kex.html#data-loading-preprocessing",
    "href": "paper/visnir-kex.html#data-loading-preprocessing",
    "title": "Visnir LSSM for Kex",
    "section": "Data loading & preprocessing",
    "text": "Data loading & preprocessing\n\ndef stat_wn_wl(wn_wl):\n    print(f'Length: {len(wn_wl)}, resolution: {wn_wl[-2]- wn_wl[-1]}, max: {np.max(wn_wl)}, min: {np.min(wn_wl)}')\n\n\n# K-spiking experiment data\nX_nir_spik, wavenumbers_spik, names_spik = fc.load_pickle('../../_data/nir-k-spiking.pkl')\n\n\nstat_wn_wl(wavenumbers_spik)\n\nLength: 949, resolution: 8, max: 11536, min: 3952\n\n\n\nanalytes = 'k.ext_usda.a725_cmolc.kg'\nspectra_type = 'visnir'\n\ndata = load_ossl(analytes, spectra_type)\nX, y, X_names, smp_idx, ds_name, ds_label = data\nX_names = wl_to_wn(X_names)\n\nReading & selecting data ...\nCPU times: user 26.3 s, sys: 4.48 s, total: 30.8 s\nWall time: 32 s\n\n\n\nstat_wn_wl(X_names)\n\nLength: 1051, resolution: 3.2025620496397096, max: 25000.0, min: 4000.0\n\n\n\nX = Pipeline([\n    ('interpolate', Interpolate(X_names, wavenumbers_spik)),\n    # ('mean center', MeanCenter()),\n    ('snv', SNV())\n]).fit_transform(X)\n\nX_names = wavenumbers_spik\ny = Log1p().fit_transform(y)\n\nFalse\n\n\n\nplot_spectra(X_t, wavenumbers_spik, alpha=0.5, ylabel='Absorbance', ascending=False)\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nfor i, ds in enumerate(ds_label): print(f'{ds_label[i]}: {len(X[ds_name == i])}')\n\nICRAF.ISRIC: 3674\nKSSL.SSL: 51\nLUCAS.SSL: 40175\nLUCAS.WOODWELL.SSL: 589\n\n\n\n# Train/valid split\nn_smp = None  # For demo. purpose\nX_train, X_valid, y_train, y_valid = train_test_split(X[:n_smp, :], y[:n_smp],\n                                                      test_size=0.1,\n                                                      stratify=ds_name[:n_smp],\n                                                      random_state=41)\n\n# Get PyTorch datasets\ntrain_ds, valid_ds = [SpectralDataset(X, y, )\n                      for X, y, in [(X_train, y_train), (X_valid, y_valid)]]\n\n\nX_train.shape\n\n(40040, 949)\n\n\n\n# Then PyTorch dataloaders\ndls = get_dls(train_ds, valid_ds, bs=64)",
    "crumbs": [
      "Paper with code",
      "Visnir LSSM for Kex"
    ]
  },
  {
    "objectID": "paper/visnir-kex.html#training",
    "href": "paper/visnir-kex.html#training",
    "title": "Visnir LSSM for Kex",
    "section": "Training",
    "text": "Training\n\ndef set_grad(m, b):\n    if isinstance(m, (nn.Linear,nn.BatchNorm2d)): return\n    if hasattr(m, 'weight'):\n        for p in m.parameters(): p.requires_grad_(b)\n\n\ndef get_n_params(model, trainable=True):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad == trainable)\n\n\nLearning from scratch\n\nmodel_name = 'resnet18'\nmodel = timm.create_model(model_name, pretrained=False,\n                          in_chans=1, num_classes=1)\n\n\n# Define modelling pipeline & Train\nepochs = 10\nlr = 5e-4\n\n# Define metrics\nmetrics = MetricsCB(r2=R2Score())\n\n# Define scheduler (how to vary learning rate during training)\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n\n# Callbacks\nxtra = [BatchSchedCB(sched)]\ngadf = BatchTransformCB(GADFTfm())\nresize = BatchTransformCB(_resizeTfm)\nstats = BatchTransformCB(StatsTfm(model.default_cfg))\n\ncbs = [DeviceCB(), gadf, resize, stats, TrainCB(),\n       metrics, ProgressCB(plot=False)]\n\nlearn = Learner(model, dls, nn.HuberLoss(), lr=lr,\n                cbs=cbs+xtra, opt_func=optim.AdamW)\n\n\nget_n_params(learn.model, trainable=True)\n\n11170753\n\n\n\nlearn.fit(epochs)\n\n\n\nNaive fine-tuning\n\nmodel_name = 'resnet18'\nmodel = timm.create_model(model_name, pretrained=True,\n                          in_chans=1, num_classes=1)\n\n# Define modelling pipeline & Train\nepochs = 5\nlr = 5e-4\n\n# Define metrics\nmetrics = MetricsCB(r2=R2Score())\n\n# Define scheduler (how to vary learning rate during training)\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n\n# Callbacks\nxtra = [BatchSchedCB(sched)]\ngadf = BatchTransformCB(GADFTfm())\nresize = BatchTransformCB(_resizeTfm)\nstats = BatchTransformCB(StatsTfm(model.default_cfg))\n\ncbs = [DeviceCB(), gadf, resize, stats, TrainCB(),\n       metrics, ProgressCB(plot=False)]\n\nlearn = Learner(model, dls, nn.HuberLoss(), lr=lr,\n                cbs=cbs+xtra, opt_func=optim.AdamW)\n\n\nget_n_params(learn.model, trainable=True)\n\n11170753\n\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nr2\nloss\nepoch\ntrain\n\n\n\n\n0.455\n0.038\n0\ntrain\n\n\n0.439\n0.040\n0\neval\n\n\n0.652\n0.025\n1\ntrain\n\n\n0.551\n0.033\n1\neval\n\n\n0.734\n0.019\n2\ntrain\n\n\n0.531\n0.034\n2\neval\n\n\n0.804\n0.014\n3\ntrain\n\n\n0.790\n0.015\n3\neval\n\n\n0.860\n0.010\n4\ntrain\n\n\n0.804\n0.014\n4\neval\n\n\n\n\n\nCPU times: user 8min 12s, sys: 2.64 s, total: 8min 15s\nWall time: 8min 12s\n\n\n\ntorch.save(learn.model.state_dict(), 'resnet-pretrained-02072024-mir.pth')\n\n\nPredicting\n\nmodel_name = 'resnet18'\nmodel = timm.create_model(model_name, pretrained=True,\n                          in_chans=1, num_classes=1)\n\nfname_model = \"../models/resnet-pretrained-03072024-mir-k-spiking.pth\"\nmodel.load_state_dict(torch.load(fname_model, map_location=torch.device('cpu')))\n\n# Define modelling pipeline & Train\nepochs = 1\nlr = 5e-4\n\n# Define metrics\nmetrics = MetricsCB(r2=R2Score())\n\n# Define scheduler (how to vary learning rate during training)\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n\n# Callbacks\nxtra = [BatchSchedCB(sched)]\ngadf = BatchTransformCB(GADFTfm())\nresize = BatchTransformCB(_resizeTfm)\nstats = BatchTransformCB(StatsTfm(model.default_cfg))\n\ncbs = [DeviceCB(device='cpu'), gadf, resize, stats, TrainCB(),\n       metrics]\n\nlearn = Learner(model, dls, nn.HuberLoss(), lr=lr,\n                cbs=cbs+xtra, opt_func=optim.AdamW)\n\n\nlearn.fit(epochs)\n\n\n# learn.model = model\n\n\n# X_mir_spik, wavenumbers_spik, names_spik = fc.load_pickle('../../_data/mir-k-spiking.pkl')\n\n\nnames_spik\n\narray(['LUI-0-0', 'LUI-0-1', 'LUI-0-2', 'LUI-0-3', 'LUI-0-4', 'LUI-1-0',\n       'LUI-1-1', 'LUI-1-2', 'LUI-1-3', 'LUI-1-4', 'LUI-2-0', 'LUI-2-1',\n       'LUI-2-2', 'LUI-2-3', 'LUI-2-4', 'LUI-3-0', 'LUI-3-1', 'LUI-3-2',\n       'LUI-3-3', 'LUI-3-4', 'SPA1-0-0', 'SPA1-0-2', 'SPA1-0-3',\n       'SPA1-0-4', 'SPA1-1-0', 'SPA1-1-1', 'SPA1-1-2', 'SPA1-1-3',\n       'SPA1-1-4', 'SPA1-2-0', 'SPA1-2-1', 'SPA1-2-2', 'SPA1-2-3',\n       'SPA1-2-4', 'SPA1-3-0', 'SPA1-3-1', 'SPA1-3-2', 'SPA1-3-3',\n       'SPA1-3-4', 'TM4.1-0-0', 'TM4.1-0-1', 'TM4.1-0-2', 'TM4.1-0-3',\n       'TM4.1-0-4', 'TM4.1-0-5', 'TM4.1-1-0', 'TM4.1-1-1', 'TM4.1-1-2',\n       'TM4.1-1-3', 'TM4.1-2-0', 'TM4.1-2-1', 'TM4.1-2-2', 'TM4.1-2-3',\n       'TM4.1-3-0', 'TM4.1-3-1', 'TM4.1-3-2', 'TM4.1-3-3', 'TM4.1-3-4'],\n      dtype='&lt;U9')\n\n\n\nnp.mean(np.expm1(y))\n\n0.6474079249216373\n\n\n\ndef predict(X, substring, names, learn):\n    mask = [substring in name for name in names_spik]\n    return learn.get_preds(X[mask], y_tfm_fn=np.expm1)\n\nfor smp_name in ['LUI', 'SPA1', 'TM4.1']:\n    print(f'Sample name: {smp_name}')\n    for substring in [f'{smp_name}-{i}' for i in range(4)]:\n        preds = predict(X_mir_spik, substring, names_spik, learn)\n        print(f'Spiking level: {substring}, mean: {np.mean(preds):.3}, std: {np.std(preds):.3}')\n    print(80*'-')\n\nSample name: LUI\nSpiking level: LUI-0, mean: 0.238, std: 0.02\nSpiking level: LUI-1, mean: 0.266, std: 0.0293\nSpiking level: LUI-2, mean: 0.179, std: 0.0602\nSpiking level: LUI-3, mean: 0.232, std: 0.0399\n--------------------------------------------------------------------------------\nSample name: SPA1\nSpiking level: SPA1-0, mean: 2.26, std: 0.459\nSpiking level: SPA1-1, mean: 2.29, std: 0.249\nSpiking level: SPA1-2, mean: 3.16, std: 0.974\nSpiking level: SPA1-3, mean: 2.88, std: 0.38\n--------------------------------------------------------------------------------\nSample name: TM4.1\nSpiking level: TM4.1-0, mean: 0.702, std: 0.297\nSpiking level: TM4.1-1, mean: 0.329, std: 0.22\nSpiking level: TM4.1-2, mean: 0.667, std: 0.0986\nSpiking level: TM4.1-3, mean: 0.409, std: 0.057\n--------------------------------------------------------------------------------\n\n\n\n\n\nFreezing batch norm & linear layers\n\nmodel_name = 'resnet18'\nmodel = timm.create_model(model_name, pretrained=True,\n                          in_chans=1, num_classes=1)\n\n# Define modelling pipeline & Train\nepochs = 3\nlr = 5e-4\n\n# Define metrics\nmetrics = MetricsCB(r2=R2Score())\n\n# Define scheduler (how to vary learning rate during training)\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n\n# Callbacks\nxtra = [BatchSchedCB(sched)]\ngadf = BatchTransformCB(GADFTfm())\nresize = BatchTransformCB(_resizeTfm)\nstats = BatchTransformCB(StatsTfm(model.default_cfg))\n\ncbs = [DeviceCB(), gadf, resize, stats, TrainCB(),\n       metrics, ProgressCB(plot=False)]\n\nlearn = Learner(model, dls, nn.HuberLoss(), lr=lr,\n                cbs=cbs+xtra, opt_func=optim.AdamW)\n\n\nlearn.model.apply(partial(set_grad, b=False));\n\n\nget_n_params(learn.model, trainable=True)\n\n10113\n\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nr2\nloss\nepoch\ntrain\n\n\n\n\n0.154\n0.058\n0\ntrain\n\n\n0.433\n0.041\n0\neval\n\n\n0.473\n0.037\n1\ntrain\n\n\n0.507\n0.035\n1\neval\n\n\n0.526\n0.033\n2\ntrain\n\n\n0.535\n0.033\n2\neval\n\n\n\n\n\nCPU times: user 3min 40s, sys: 4.66 s, total: 3min 44s\nWall time: 3min 39s\n\n\n\n# Now unfreeze all\nlearn.model.apply(partial(set_grad, b=True));\n\n\nget_n_params(learn.model, trainable=True)\n\n11170753\n\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nr2\nloss\nepoch\ntrain\n\n\n\n\n0.578\n0.030\n0\ntrain\n\n\n0.201\n0.057\n0\neval\n\n\n0.698\n0.021\n1\ntrain\n\n\n0.731\n0.020\n1\neval\n\n\n0.802\n0.014\n2\ntrain\n\n\n0.780\n0.016\n2\neval\n\n\n\n\n\nCPU times: user 4min 43s, sys: 5.23 s, total: 4min 48s\nWall time: 4min 42s\n\n\n\n\nLearning rate finder\n\nmodel_name = 'resnet18'\nmodel = timm.create_model(model_name, pretrained=False,\n                          in_chans=1, num_classes=1)\n\n\n# Define modelling pipeline & Train\nepochs = 3\nlr = 5e-3\n\nmetrics = MetricsCB(r2=R2Score())\n\ntmax = epochs * len(dls.train)\n\ngadf = BatchTransformCB(GADFTfm())\nresize = BatchTransformCB(_resizeTfm)\nstats = BatchTransformCB(StatsTfm(model.default_cfg))\n\ncbs = [DeviceCB(), gadf, resize, stats, TrainCB(),\n       metrics, ProgressCB(plot=False)]\n\nlearn = Learner(model, dls, nn.HuberLoss(), lr=lr,\n                cbs=cbs, opt_func=optim.AdamW)\n\n\n# gamma=1.3, max_mult=3, start_lr=1e-5, max_epochs=10\nlearn.lr_find(gamma=1.1,start_lr=1e-5, max_mult=10)\n\n\n\n\n\n\n    \n      \n      0.00% [0/10 00:00&lt;?]\n    \n    \n\n\n    \n      \n      9.98% [81/812 00:09&lt;01:21 0.125]",
    "crumbs": [
      "Paper with code",
      "Visnir LSSM for Kex"
    ]
  },
  {
    "objectID": "dataloaders.html",
    "href": "dataloaders.html",
    "title": "Dataloaders",
    "section": "",
    "text": "source\n\nget_dls\n\n get_dls (train_ds:torch.utils.data.dataset.Dataset,\n          valid_ds:torch.utils.data.dataset.Dataset, bs:int, **kwargs)\n\nTrain and valid dataloaders.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntrain_ds\nDataset\nTrain dataset\n\n\nvalid_ds\nDataset\nValid dataset\n\n\nbs\nint\nBatch size\n\n\nkwargs\n\n\n\n\nReturns\nDataLoader\nNamedTuple DataLoaderwith .train and .valid field names\n\n\n\n\nsource\n\n\nCrossSpectraDataset\n\n CrossSpectraDataset (df:pandas.core.frame.DataFrame, pair_idxs:list,\n                      spectra_at:int=2)\n\nCustom Pytorch dataset for IR instrument cross-calibration.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\nInfrared spectra with associated sample id\n\n\npair_idxs\nlist\n\nList of tuples (pairs) of replicate indices to sample from the dataframe\n\n\nspectra_at\nint\n2\nIndex of the column where the spectroscopy data starts\n\n\n\nFor example:\n\nfrom lssm.loading import get_spectra_pair_idxs\n\ndf_test = pd.DataFrame({'sample_id': [0,0,1,1], \n                        'organization': ['A', 'B', 'A', 'B'],\n                        '650': [1.2, 0.6, 0.5, 1.3],\n                        '652': [0.8, 0.4, 0.6, 1.1]\n                        }, index= [0,1,2,3])\ndf_test.index.name = 'index'; df_test\n\n\n\n\n\n\n\n\nsample_id\norganization\n650\n652\n\n\nindex\n\n\n\n\n\n\n\n\n0\n0\nA\n1.2\n0.8\n\n\n1\n0\nB\n0.6\n0.4\n\n\n2\n1\nA\n0.5\n0.6\n\n\n3\n1\nB\n1.3\n1.1\n\n\n\n\n\n\n\n\nget_spectra_pair_idxs(df_test)\n\n[(0, 0), (0, 1), (1, 0), (1, 1), (2, 2), (2, 3), (3, 2), (3, 3)]\n\n\n\nds = CrossSpectraDataset(df_test, get_spectra_pair_idxs(df_test))\n\n# Index pair: (0, 0)\nx, y = tensor([[1.2, 0.8]]), tensor([[1.2, 0.8]])\nfc.test_eq(ds[0], (x, y))\n\n# Index pair: (0, 1)\nx, y = tensor([[1.2, 0.8]]), tensor([[0.6, 0.4]])\nfc.test_eq(ds[1], (x, y))\n\n# Index pair: (1, 0)\nx, y = tensor([[0.6, 0.4]]), tensor([[1.2, 0.8]])\nfc.test_eq(ds[2], (x, y))\n\n# Index pair: (1, 1)\nx, y = tensor([[0.6, 0.4]]), tensor([[0.6, 0.4]])\nfc.test_eq(ds[3], (x, y))\n\n# Index pair: (2, 2)\nx, y = tensor([[0.5, 0.6]]), tensor([[0.5, 0.6]])\nfc.test_eq(ds[4], (x, y))\n\n# Index pair: (2, 3)\nx, y = tensor([[0.5, 0.6]]), tensor([[1.3, 1.1]])\nfc.test_eq(ds[5], (x, y))\n\n\nsource\n\n\nSpectralDataset\n\n SpectralDataset (X:numpy.ndarray, y:numpy.ndarray,\n                  metadata:numpy.ndarray=None)\n\n(Infrared Spectra, soil property) custom PyTorch Dataset.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\nndarray\n\nSpectra\n\n\ny\nndarray\n\nAnalyte\n\n\nmetadata\nndarray\nNone\n\n\n\n\nFor example, below a canonical pipeline where we:\n\nload the data\ntransform the data (here to absorbance and continuum removal)\nperform a train, test split\naccess Pytorch custom SpectralDataset\nfinally get PyTorch dataloaders ready for training\n\n\n# 1. Data loading\nanalytes = 'k.ext_usda.a725_cmolc.kg'\ndata = load_ossl(analytes, spectra_type='visnir')\nX, y, X_names, smp_idx, ds_name, ds_label = data\n\n# 2. Transform\nX = Pipeline([('to_abs', ToAbsorbance()),\n              ('cr', ContinuumRemoval(X_names))]).fit_transform(X)\n\n# 3. Train/valid split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                                      test_size=0.2,\n                                                      stratify=ds_name,\n                                                      random_state=41)\n\n# 4. Get PyTorch datasets\ntrain_ds, valid_ds = [SpectralDataset(X, y)\n                      for X, y, in [(X_train, y_train), (X_valid, y_valid)]]\n\n# 5. Then PyTorch dataloaders\ndls = get_dls(train_ds, valid_ds, bs=32)\n\nfirst_batch = next(iter(dls.train))\nprint(f'First batch X dim: {first_batch[0].shape}')\nprint(f'First batch y dim: {first_batch[1].shape}')\n\nReading & selecting data ...\n\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44489/44489 [00:15&lt;00:00, 2849.97it/s]\n\n\nFirst batch X dim: torch.Size([32, 1, 1051])\nFirst batch y dim: torch.Size([32, 1])\n\n\n\nsource\n\n\nSpectralEmbeddingDataset\n\n SpectralEmbeddingDataset (X:numpy.ndarray)\n\n(Infrared Spectra, Infrared Spectra) custom PyTorch Dataset.\n\n\n\n\nType\nDetails\n\n\n\n\nX\nndarray\nSpectra\n\n\n\n\nimport nbdev; nbdev.nbdev_export()",
    "crumbs": [
      "API",
      "Dataloaders"
    ]
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "from lssm.loading import load_ossl\n\n\nsource\n\nplot_spectra\n\n plot_spectra (X, X_names, sample=50, ascending=True, alpha=0.8,\n               color='#333', xlabel='Wavenumber', ylabel='Absorbance',\n               title=None, figsize=(20, 4))\n\nExample:\n\nanalytes = 'k.ext_usda.a725_cmolc.kg'\ndata = load_ossl(analytes, spectra_type='visnir')\nX, y, X_names, smp_idx, ds_name, ds_label = data\n\nReading & selecting data ...\n\n\n\nplot_spectra(X, X_names, alpha=0.5, ylabel='Reflectance')\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nIf a single spectrum:\n\nplot_spectra(X[0], X_names, alpha=0.5, ylabel='Reflectance', ascending=True)\n\n&lt;Figure size 640x480 with 0 Axes&gt;",
    "crumbs": [
      "API",
      "Visualization"
    ]
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "Models",
    "section": "",
    "text": "source\n\n\n\n GeneralRelu (leak=None, sub=None, maxv=None)\n\nRelu unit as implemented in Fastai miniai package: https://github.com/fastai/minai\n\nsource\n\n\n\n\n plot_func (f, start=-5.0, end=5.0, steps=100, size=(5, 3))\n\nPlot a function (e.g activation function).\n\nplot_func(GeneralRelu(leak=0.1, sub=0.4))",
    "crumbs": [
      "API",
      "Models"
    ]
  },
  {
    "objectID": "models.html#activations",
    "href": "models.html#activations",
    "title": "Models",
    "section": "",
    "text": "source\n\n\n\n GeneralRelu (leak=None, sub=None, maxv=None)\n\nRelu unit as implemented in Fastai miniai package: https://github.com/fastai/minai\n\nsource\n\n\n\n\n plot_func (f, start=-5.0, end=5.0, steps=100, size=(5, 3))\n\nPlot a function (e.g activation function).\n\nplot_func(GeneralRelu(leak=0.1, sub=0.4))",
    "crumbs": [
      "API",
      "Models"
    ]
  },
  {
    "objectID": "models.html#initialization",
    "href": "models.html#initialization",
    "title": "Models",
    "section": "Initialization",
    "text": "Initialization\n\nsource\n\ninit_weights\n\n init_weights (m, leaky=0.0)\n\nInitialize weights using kaiming normal method if Conv1d, …, Linear.",
    "crumbs": [
      "API",
      "Models"
    ]
  },
  {
    "objectID": "models.html#layers",
    "href": "models.html#layers",
    "title": "Models",
    "section": "Layers",
    "text": "Layers\n\nsource\n\nconv\n\n conv (ni:int, nf:int, ks:int=3, stride:int=2, dim:int=1,\n       act:torch.nn.modules.module.Module=&lt;class\n       'torch.nn.modules.activation.ReLU'&gt;,\n       norm:torch.nn.modules.module.Module=None, bias:bool=None)\n\nWrapping torch.Conv1D or torch.Conv2D.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nni\nint\n\nNumber of input channels\n\n\nnf\nint\n\nNumber of output channels\n\n\nks\nint\n3\nKernel size\n\n\nstride\nint\n2\nStride\n\n\ndim\nint\n1\nConvolution dimension (1 or 2)\n\n\nact\nModule\nReLU\nActivation function\n\n\nnorm\nModule\nNone\nNormalization layer type\n\n\nbias\nbool\nNone\nIs bias needed?\n\n\n\nFor example:\n\nconv(ni=1, nf=16)(torch.rand((1,1,1500)))\n\ntensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0127],\n         [0.6139, 0.6225, 0.5387,  ..., 0.9317, 0.4794, 0.6125],\n         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n         ...,\n         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n         [0.1748, 0.0568, 0.1272,  ..., 0.1225, 0.0139, 0.0000],\n         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n       grad_fn=&lt;ReluBackward0&gt;)",
    "crumbs": [
      "API",
      "Models"
    ]
  },
  {
    "objectID": "models.html#variational-autoencoder-vae",
    "href": "models.html#variational-autoencoder-vae",
    "title": "Models",
    "section": "Variational Autoencoder (VAE)",
    "text": "Variational Autoencoder (VAE)\n1D Convolutional VAE using a Resnet Encoder and decoder.\n\nsource\n\nconv_block\n\n conv_block (ni:int, nf:int, ks:int=3, stride:int=2,\n             act:torch.nn.modules.module.Module=&lt;class\n             'torch.nn.modules.activation.ReLU'&gt;,\n             norm:torch.nn.modules.module.Module=None)\n\nInitial Resnet conv block.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nni\nint\n\nNumber of input channels\n\n\nnf\nint\n\nNumber of output channels\n\n\nks\nint\n3\nKernel size\n\n\nstride\nint\n2\nStride\n\n\nact\nModule\nReLU\nActivation function\n\n\nnorm\nModule\nNone\nNormalization layer type\n\n\n\nFor example:\n\nconv_block(ni=1, nf=16, stride=1)(torch.rand((1,1, 1500))).shape\n\ntorch.Size([1, 16, 1500])\n\n\n\nsource\n\n\nResBlock\n\n ResBlock (ni:int, nf:int, ks:int=3, stride:int=2,\n           act:torch.nn.modules.module.Module=&lt;class\n           'torch.nn.modules.activation.ReLU'&gt;,\n           norm:torch.nn.modules.module.Module=None)\n\n1D Residual block.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nni\nint\n\nNumber of input channels\n\n\nnf\nint\n\nNumber of output channels\n\n\nks\nint\n3\nKernel size\n\n\nstride\nint\n2\nStride\n\n\nact\nModule\nReLU\nActivation function\n\n\nnorm\nModule\nNone\nNormalization layer type\n\n\n\nFor example:\n\nResBlock(ni=1, nf=16, stride=1)(torch.rand((1,1, 1500))).shape\n\ntorch.Size([1, 16, 1500])\n\n\n\nsource\n\n\nResBlockUp\n\n ResBlockUp (ni:int, nf:int, ks:int=3, stride:int=2,\n             act:torch.nn.modules.module.Module=&lt;class\n             'torch.nn.modules.activation.ReLU'&gt;,\n             norm:torch.nn.modules.module.Module=None)\n\n1D Residual block including upsampling for a VAE decoder.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nni\nint\n\nNumber of input channels\n\n\nnf\nint\n\nNumber of output channels\n\n\nks\nint\n3\nKernel size\n\n\nstride\nint\n2\nStride\n\n\nact\nModule\nReLU\nActivation function\n\n\nnorm\nModule\nNone\nNormalization layer type\n\n\n\nFor example:\n\nResBlockUp(ni=1, nf=16, stride=1)(torch.rand((1, 1, 256))).shape\n\ntorch.Size([1, 16, 512])\n\n\n\nsource\n\n\nEncoder\n\n Encoder (act:torch.nn.modules.module.Module=&lt;class\n          'torch.nn.modules.activation.ReLU'&gt;, nfs:tuple=(8, 16, 32, 64,\n          128, 256), norm:torch.nn.modules.module.Module=None,\n          z_dim:int=2)\n\nConvolutional VAE Encoder.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nact\nModule\nReLU\nActivation function\n\n\nnfs\ntuple\n(8, 16, 32, 64, 128, 256)\nSuccessive output channels size\n\n\nnorm\nModule\nNone\nNormalization layer type\n\n\nz_dim\nint\n2\nLatent space dimension\n\n\n\nFor example:\n\nmu, logvar = Encoder()(torch.rand((1,1, 1500))); mu\n\ntensor([[-0.0564,  0.0030]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nsource\n\n\nDecoder\n\n Decoder (in_shape:int, act:torch.nn.modules.module.Module=&lt;class\n          'torch.nn.modules.activation.ReLU'&gt;, nfs:tuple=(8, 16, 32, 64,\n          128, 256), norm:torch.nn.modules.module.Module=None,\n          z_dim:int=2)\n\nConvolutional VAE Decoder.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_shape\nint\n\nLength of input (e.g a infrared spectra with 1676 wavenumbers)\n\n\nact\nModule\nReLU\nActivation function\n\n\nnfs\ntuple\n(8, 16, 32, 64, 128, 256)\nSuccessive output channels size\n\n\nnorm\nModule\nNone\nNormalization layer type\n\n\nz_dim\nint\n2\nLatent space dimension\n\n\n\n\nDecoder(in_shape=1600)(torch.rand((1,2))).shape\n\ntorch.Size([1, 1, 1632])\n\n\n\nsource\n\n\nCVAE\n\n CVAE (in_shape:int, z_dim:int=2, nfs:tuple=(8, 16, 32, 64, 128, 256))\n\n1D Resnet-based convolutional variational autoencoder.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_shape\nint\n\nLength of input (e.g a infrared spectra with 1676 wavenumbers)\n\n\nz_dim\nint\n2\nLatent space dimension\n\n\nnfs\ntuple\n(8, 16, 32, 64, 128, 256)\nSuccessive output channels size\n\n\n\nFor example:\n\ninput_reconstructed, _, _ = CVAE(1500)(torch.rand((1,1, 1500)))\ninput_reconstructed.shape\n\ntorch.Size([1, 1, 1500])\n\n\n\nimport nbdev; nbdev.nbdev_export()",
    "crumbs": [
      "API",
      "Models"
    ]
  },
  {
    "objectID": "loading.html",
    "href": "loading.html",
    "title": "Loading",
    "section": "",
    "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nfrom lssm.visualization import plot_spectra",
    "crumbs": [
      "API",
      "Loading"
    ]
  },
  {
    "objectID": "loading.html#large-spectral-libraries",
    "href": "loading.html#large-spectral-libraries",
    "title": "Loading",
    "section": "Large spectral libraries",
    "text": "Large spectral libraries\nKSSL, LUCAS, … larger spectral library are now accessible through a single API endpoint provided by OSSL.\n\nsource\n\ndownload\n\n download (url:str, dest_dir:str)\n\nDownload data available at url into the dest directory (creates it on the way if does not exist).\n\n\n\n\nType\nDetails\n\n\n\n\nurl\nstr\nurl to dowload data from\n\n\ndest_dir\nstr\ndirectory to download data to\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nload_ossl\n\n load_ossl (analytes:Union[str,List[str]], spectra_type:str='visnir',\n            src:pathlib.Path=Path('/home/runner/.lssm/data/ossl'),\n            debug:bool=False)\n\nLoad all available OSSL data and filter it by spectra type and analytes of interest\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nanalytes\nUnion\n\nUsing OSSL’s analytes naming conventions\n\n\nspectra_type\nstr\nvisnir\nPossible values: ‘mir’, ‘visnir’\n\n\nsrc\nPath\n/home/runner/.lssm/data/ossl\ndirectory containing the data\n\n\ndebug\nbool\nFalse\nreturn unprocessed loaded data directly for further investigation\n\n\n\nExample with Near-infrared data:\n\nanalytes = 'k.ext_usda.a725_cmolc.kg'\ndata = load_ossl(analytes, spectra_type='visnir')\nX, y, X_names, smp_idx, ds_name, ds_label = data\n\nprint(X.shape, y.shape)\n\nReading & selecting data ...\n(44489, 1051) (44489, 1)\n\n\nOr with Mid-infrared ones:\n\nanalytes = 'k.ext_usda.a725_cmolc.kg'\ndata = load_ossl(analytes, spectra_type='mir')\nX, y, X_names, smp_idx, ds_name, ds_label = data\n\nReading & selecting data ...\n\n\n\n_, counts = np.unique(ds_name, return_counts=True)\n\n# Number of data points per data provider:\nlist(zip(ds_label, counts))\n\n[('GARRETT.SSL', 183),\n ('ICRAF.ISRIC', 3672),\n ('KSSL.SSL', 53230),\n ('LUCAS.WOODWELL.SSL', 589)]",
    "crumbs": [
      "API",
      "Loading"
    ]
  },
  {
    "objectID": "loading.html#ossl-mir-ring-trial-data",
    "href": "loading.html#ossl-mir-ring-trial-data",
    "title": "Loading",
    "section": "OSSL MIR ring trial data",
    "text": "OSSL MIR ring trial data\n\nsource\n\nload_mir_ring_trial\n\n load_mir_ring_trial (fname)\n\nExample:\n\nfname = Path().home() / \\\n    'pro/data/woodwell-ringtrial/drive-download-20231013T123706Z-001/RT_STD_allMIRspectra_raw.csv'\ndf = load_mir_ring_trial(fname)\ndf.head()\n\n\n\n\n\n\n\n\norganization\nsample_id\n650\n652\n654\n656\n658\n660\n662\n664\n...\n3982\n3984\n3986\n3988\n3990\n3992\n3994\n3996\n3998\n4000\n\n\n\n\n0\nAgrocares\nRT_01\n1.97644\n1.97292\n1.97015\n1.96778\n1.96656\n1.96609\n1.96656\n1.96773\n...\n1.09917\n1.09898\n1.09877\n1.09851\n1.09827\n1.09811\n1.09797\n1.09797\n1.09797\n1.09797\n\n\n1\nAgrocares\nRT_02\n2.48977\n2.50440\n2.51282\n2.51789\n2.51765\n2.51403\n2.50914\n2.50330\n...\n1.24065\n1.24042\n1.24015\n1.23977\n1.23938\n1.23896\n1.23861\n1.23861\n1.23861\n1.23861\n\n\n2\nAgrocares\nRT_03\n2.63074\n2.63082\n2.63692\n2.64624\n2.64827\n2.64566\n2.63292\n2.61260\n...\n1.25036\n1.24959\n1.24877\n1.24777\n1.24679\n1.24585\n1.24505\n1.24505\n1.24505\n1.24505\n\n\n3\nAgrocares\nRT_04\n2.27312\n2.26257\n2.25124\n2.23949\n2.23315\n2.23026\n2.22792\n2.22598\n...\n1.15192\n1.15176\n1.15158\n1.15132\n1.15106\n1.15080\n1.15058\n1.15058\n1.15058\n1.15058\n\n\n4\nAgrocares\nRT_05\n2.26744\n2.25584\n2.24658\n2.23859\n2.23159\n2.22521\n2.22157\n2.22000\n...\n1.16109\n1.16106\n1.16102\n1.16095\n1.16086\n1.16070\n1.16056\n1.16056\n1.16056\n1.16056\n\n\n\n\n5 rows × 1678 columns\n\n\n\n\nsource\n\n\nget_spectra_pair_idxs\n\n get_spectra_pair_idxs (df)\n\nRetrieve index pairs of replicated spectra, representing measurements taken from the same soil sample but using different instruments.\nExample:\n\ndf_test = pd.DataFrame({'sample_id': [0,0,1,1]}, index= [0,1,2,3]); df_test\n\n\n\n\n\n\n\n\nsample_id\n\n\n\n\n0\n0\n\n\n1\n0\n\n\n2\n1\n\n\n3\n1\n\n\n\n\n\n\n\n\nexpected = [(0, 0), (0, 1), (1, 0), (1, 1), (2, 2), (2, 3), (3, 2), (3, 3)]\nfc.test_eq(get_spectra_pair_idxs(df_test), expected)",
    "crumbs": [
      "API",
      "Loading"
    ]
  },
  {
    "objectID": "loading.html#mir-nir-kex-spiking-experiment",
    "href": "loading.html#mir-nir-kex-spiking-experiment",
    "title": "Loading",
    "section": "MIR & NIR Kex spiking experiment",
    "text": "MIR & NIR Kex spiking experiment\n\nsource\n\nload_mir_kex_spike\n\n load_mir_kex_spike (src_dir)\n\n*Load MIR spectra of K spiked soil samples.\nParameters: src_dir (Path-like object): Directory containing the spectra files.\nReturns: tuple: Tuple containing the array of absorbance values, array of wavenumbers (columns), and array of sample names (rows).*\n\n# def load_mir_kex_spike(src_dir): \n#     \"Load MIR spectra of K spiked soil samples\"\n#     pattern = r'-\\d-\\d$'\n#     fnames = [f for f in src_dir.ls() if re.search(pattern, f.stem)]\n#     dfs = []\n#     for fname in fnames:\n#         df = pd.read_csv(fname, header=None, names=['wavenumber', 'absorbance'])\n#         df = df[(df.wavenumber &gt; 649) & (df.wavenumber &lt; 4000)]\n#         df['name'] = fname.stem\n#         dfs.append(df)\n#     df = pd.concat(dfs).pivot_table(values='absorbance', index='name', columns='wavenumber')\n#     return df.values, df_mir_spiked.columns.values, df_mir_spiked.index.values.astype('U')\n\nFor example:\n\nsrc_dir = Path().home() / 'pro/data/k-spiking/mir'\nX, wavenumbers, names = load_mir_kex_spike(src_dir)\n\n\nmask_smp = np.char.find(names, 'TM') == 0\nplot_spectra(X[mask_smp,:], wavenumbers, ascending=False)\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nload_nir_kex_spike\n\n load_nir_kex_spike (fname)\n\n*Load NIR spectra of K spiked soil samples.\nParameters: fname (str or Path-like object): File name or path of the Excel file.\nReturns: tuple: Tuple containing the array of spectral values, array of wavenumbers (columns), and array of sample names (rows).*\n\nfname = Path().home() / 'pro/data/k-spiking/nir/2023-12-8 _FT-NIR-K-spiked soil.xlsx'\nX, wavenumbers, names = load_nir_kex_spike(fname)\n\n\nmask_smp = np.char.find(names, 'TM') == 0\nplot_spectra(X[mask_smp,:], wavenumbers, ascending=False)\n\n&lt;Figure size 640x480 with 0 Axes&gt;",
    "crumbs": [
      "API",
      "Loading"
    ]
  },
  {
    "objectID": "callbacks.html",
    "href": "callbacks.html",
    "title": "Callbacks",
    "section": "",
    "text": "source\n\nto_device\n\n to_device (x, device='cpu')\n\n\nsource\n\n\nto_cpu\n\n to_cpu (x)\n\n\nsource\n\n\nCallback\n\n Callback ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nrun_cbs\n\n run_cbs (cbs:list, method_nm:str, learn=None)\n\nRun callbacks in the order defined in their order class attribute.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncbs\nlist\n\nCallbacks to run\n\n\nmethod_nm\nstr\n\nName of the method to call on all passed callbacks\n\n\nlearn\nNoneType\nNone\nContect object in which the callbacks might be used\n\n\n\nFor example:\n\nclass Context:\n    \"Context in which the callbacks might be used.\"\n    def __init__(self):\n        self.str = 'I am {} and should be printed second.'\n\nclass CB: \n    order = 0\n    def whomai(self, learn): \n        name = self.__class__.__name__\n        print(learn.str.format(name))\n    \nclass CB1(CB): order= 2\nclass CB2(CB): order= 1\n    \nrun_cbs([CB1(), CB2()], 'whomai', Context())\n\nI am CB2 and should be printed second.\nI am CB1 and should be printed second.\n\n\nSilently ignores method if does not exist:\n\nrun_cbs([CB1(), CB2()], 'donotexist', Context())\n\n\nsource\n\n\nBaseSchedCB\n\n BaseSchedCB (sched)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nBatchSchedCB\n\n BatchSchedCB (sched)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nBatchTransformCB\n\n BatchTransformCB (tfm, on_train=True, on_val=True)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nDeviceCB\n\n DeviceCB (device:str='cpu')\n\nPut model (before_fit) or batch (before_batch) to device.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndevice\nstr\ncpu\n‘cpu, ’mps’, ‘cuda’, …\n\n\n\n\nsource\n\n\nMetricsCB\n\n MetricsCB (*ms, **metrics)\n\nMetrics callback.\n\nsource\n\n\nProgressCB\n\n ProgressCB (plot=False)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nSingleBatchCB\n\n SingleBatchCB ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nTrainCB\n\n TrainCB (n_inp=1)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nCancelEpochException\nCommon base class for all non-exit exceptions.\n\nsource\n\n\nCancelBatchException\nCommon base class for all non-exit exceptions.\n\nsource\n\n\nCancelFitException\nCommon base class for all non-exit exceptions.\n\nsource\n\n\nLRFinderCB\n\n LRFinderCB (gamma=1.3, max_mult=3)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nimport nbdev; nbdev.nbdev_export()",
    "crumbs": [
      "API",
      "Callbacks"
    ]
  },
  {
    "objectID": "transforms.html",
    "href": "transforms.html",
    "title": "Transforms",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nfrom lssm.loading import load_ossl\nfrom lssm.preprocessing import ToAbsorbance, ContinuumRemoval\nfrom lssm.dataloaders import SpectralDataset, get_dls\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\n\nsource\n\nGADFTfm\n\n GADFTfm (neg=True)\n\n*Transform batch of spectra S (B, 1, len(S)) into their Grammian Difference Matrix Field (GADF) of shape (B, 1, H, W)\nNotes: https://arxiv.org/pdf/1506.00327.pdf*\nExample:\n\n# Load data\nanalytes = 'k.ext_usda.a725_cmolc.kg'\ndata = load_ossl(analytes, spectra_type='visnir')\nX, y, X_names, smp_idx, ds_name, ds_label = data\n\n# Transform\nX = Pipeline([('to_abs', ToAbsorbance()), \n              ('cr', ContinuumRemoval(X_names))]).fit_transform(X)\n\n# Train/valid split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                      test_size=0.2,\n                                                      stratify=ds_name, \n                                                      random_state=41)\n\n# Get PyTorch datasets\ntrain_ds, valid_ds = [SpectralDataset(X, y, ) \n                      for X, y, in [(X_train, y_train), (X_valid, y_valid)]]\n\n# Then PyTorch dataloaders\ndls = get_dls(train_ds, valid_ds, bs=32)\n\nfirst_batch = next(iter(dls.train))\n\nReading & selecting data ...\n\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44489/44489 [00:15&lt;00:00, 2854.63it/s]\n\n\n\nplt.imshow(GADFTfm()(first_batch)[0][0].squeeze().cpu(),\n           cmap='Spectral', \n           origin='upper', extent=[X_names[0], X_names[-1], X_names[-1], X_names[0]]);\n\n\n\n\n\n\n\n\n\nprint(_resizeTfm(GADFTfm()(first_batch))[0].shape)\nplt.imshow(_resizeTfm(GADFTfm()(first_batch), size=32)[0][0].squeeze().cpu(),\n           cmap='Spectral', \n           origin='upper', extent=[X_names[0], X_names[-1], X_names[-1], X_names[0]]);\n\ntorch.Size([32, 1, 224, 224])\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nStatsTfm\n\n StatsTfm (cfgs)\n\nSet pre-trained statistics.\n\nsource\n\n\nSNVTfm\n\n SNVTfm (is_VAE:bool=False)\n\nApply SNV to input or to both input and output when used with a Variational Auto-Encoder.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nis_VAE\nbool\nFalse\nIf VAE apply SNV on both input and output.\n\n\n\nFor instance, when X (input) is a 1D spectrum and the output is a single soil property:\n\nbatch = torch.randn(32, 1, 1500), torch.randn(32, 1)\nbatch_transformed = SNVTfm(is_VAE=False)(batch)\n\nfc.test_eq(batch_transformed[0].shape, [32, 1, 1500])\nfc.test_eq(batch_transformed[1].shape, [32, 1])\n\n\nbatch = torch.randn(32, 1, 1500), torch.randn(32, 1, 1500)\nbatch_transformed = SNVTfm(is_VAE=True)(batch)\n\nfc.test_eq(batch_transformed[0].shape, [32, 1, 1500])\nfc.test_eq(batch_transformed[1].shape, [32, 1, 1500])\n\n\nimport nbdev; nbdev.nbdev_export()",
    "crumbs": [
      "API",
      "Transforms"
    ]
  },
  {
    "objectID": "learner.html",
    "href": "learner.html",
    "title": "Learner",
    "section": "",
    "text": "Generic and highly flexible learner given the ability to inject behaviour almost every in the pipeline using callbacks. For further details and to give credit where credit is due, please refers to: FastAI course part II.\nsource",
    "crumbs": [
      "API",
      "Learner"
    ]
  },
  {
    "objectID": "learner.html#hooks",
    "href": "learner.html#hooks",
    "title": "Learner",
    "section": "Hooks",
    "text": "Hooks\nWIP\n\nimport nbdev; nbdev.nbdev_export()",
    "crumbs": [
      "API",
      "Learner"
    ]
  },
  {
    "objectID": "paper/visnir-embeddings.html",
    "href": "paper/visnir-embeddings.html",
    "title": "VisNIR embedding",
    "section": "",
    "text": "from pathlib import Path\nfrom functools import partial\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\nimport timm\nfrom torcheval.metrics import R2Score\nfrom torch.optim import lr_scheduler\nfrom torch import optim, nn\n\nfrom lssm.loading import load_ossl\nfrom lssm.preprocessing import ToAbsorbance, ContinuumRemoval, Log1p\nfrom lssm.dataloaders import SpectralDataset, get_dls\nfrom lssm.callbacks import (MetricsCB, BatchSchedCB, BatchTransformCB,\n                            DeviceCB, TrainCB, ProgressCB)\nfrom lssm.transforms import GADFTfm, _resizeTfm, StatsTfm\nfrom lssm.learner import Learner\n\n/Users/franckalbinet/mambaforge/envs/lssm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm"
  },
  {
    "objectID": "paper/visnir-embeddings.html#imports",
    "href": "paper/visnir-embeddings.html#imports",
    "title": "VisNIR embedding",
    "section": "",
    "text": "from pathlib import Path\nfrom functools import partial\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\nimport timm\nfrom torcheval.metrics import R2Score\nfrom torch.optim import lr_scheduler\nfrom torch import optim, nn\n\nfrom lssm.loading import load_ossl\nfrom lssm.preprocessing import ToAbsorbance, ContinuumRemoval, Log1p\nfrom lssm.dataloaders import SpectralDataset, get_dls\nfrom lssm.callbacks import (MetricsCB, BatchSchedCB, BatchTransformCB,\n                            DeviceCB, TrainCB, ProgressCB)\nfrom lssm.transforms import GADFTfm, _resizeTfm, StatsTfm\nfrom lssm.learner import Learner\n\n/Users/franckalbinet/mambaforge/envs/lssm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm"
  },
  {
    "objectID": "paper/visnir-embeddings.html#data-loading-preprocessing",
    "href": "paper/visnir-embeddings.html#data-loading-preprocessing",
    "title": "VisNIR embedding",
    "section": "Data loading & preprocessing",
    "text": "Data loading & preprocessing\n\nanalytes = 'k.ext_usda.a725_cmolc.kg'\nspectra_type = 'visnir'\n\ndata = load_ossl(analytes, spectra_type)\nX, y, X_names, smp_idx, ds_name, ds_label = data\n\nX = Pipeline([('to_abs', ToAbsorbance()),\n              ('cr', ContinuumRemoval(X_names))]).fit_transform(X)\ny = Log1p().fit_transform(y)\n\nReading & selecting data ...\n\n\n100%|██████████| 44489/44489 [00:16&lt;00:00, 2775.13it/s]\n\n\n\n# Train/valid split\nn_smp = None  # For demo. purpose\nX_train, X_valid, y_train, y_valid = train_test_split(X[:n_smp, :], y[:n_smp],\n                                                      test_size=0.1,\n                                                      stratify=ds_name[:n_smp],\n                                                      random_state=41)\n\n# Get PyTorch datasets\ntrain_ds, valid_ds = [SpectralDataset(X, y, )\n                      for X, y, in [(X_train, y_train), (X_valid, y_valid)]]\n\n# Then PyTorch dataloaders\ndls = get_dls(train_ds, valid_ds, bs=32)"
  },
  {
    "objectID": "paper/visnir-embeddings.html#dl-model-ensembling",
    "href": "paper/visnir-embeddings.html#dl-model-ensembling",
    "title": "VisNIR embedding",
    "section": "DL model ensembling",
    "text": "DL model ensembling\n\nmodel_name = 'resnet18'\nmodel = timm.create_model(model_name, pretrained=True,\n                          in_chans=1, num_classes=1)\n\n# Define modelling pipeline & Train\nepochs = 1\nlr = 5e-3\n\nmetrics = MetricsCB(r2=R2Score())\n\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n\nxtra = [BatchSchedCB(sched)]\n\ngadf = BatchTransformCB(GADFTfm())\nresize = BatchTransformCB(_resizeTfm)\nstats = BatchTransformCB(StatsTfm(model.default_cfg))\n\ncbs = [DeviceCB(), gadf, resize, stats, TrainCB(),\n       metrics, ProgressCB(plot=False)]\n\nlearn = Learner(model, dls, nn.MSELoss(), lr=lr,\n                cbs=cbs+xtra, opt_func=optim.AdamW)\n\n\nlearn.fit(epochs)"
  },
  {
    "objectID": "paper/mir-kex.html",
    "href": "paper/mir-kex.html",
    "title": "MIR LSSM for Kex",
    "section": "",
    "text": "# !pip install --upgrade lssm\nimport lssm\nlssm.__version__\n\n'0.0.9'"
  },
  {
    "objectID": "paper/mir-kex.html#imports",
    "href": "paper/mir-kex.html#imports",
    "title": "MIR LSSM for Kex",
    "section": "Imports",
    "text": "Imports\n\nfrom pathlib import Path\nfrom functools import partial\nimport fastcore.all as fc\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\n\nimport timm\nimport torch\nfrom torcheval.metrics import R2Score\nfrom torch.optim import lr_scheduler\nfrom torch import optim, nn\n\nfrom lssm.loading import load_ossl\nfrom lssm.preprocessing import ToAbsorbance, ContinuumRemoval, Log1p, SNV, Interpolate\nfrom lssm.dataloaders import SpectralDataset, get_dls\nfrom lssm.callbacks import (MetricsCB, BatchSchedCB, BatchTransformCB,\n                            DeviceCB, TrainCB, ProgressCB)\nfrom lssm.transforms import GADFTfm, _resizeTfm, StatsTfm\nfrom lssm.learner import Learner\nfrom lssm.visualization import plot_spectra"
  },
  {
    "objectID": "paper/mir-kex.html#data-loading-preprocessing",
    "href": "paper/mir-kex.html#data-loading-preprocessing",
    "title": "MIR LSSM for Kex",
    "section": "Data loading & preprocessing",
    "text": "Data loading & preprocessing\n\n# K-spiking experiment data\nX_mir_spik, wavenumbers_spik, names_spik = fc.load_pickle('../../_data/mir-k-spiking.pkl')\n\n\nanalytes = 'k.ext_usda.a725_cmolc.kg'\nspectra_type = 'mir'\n\ndata = load_ossl(analytes, spectra_type)\nX, y, X_names, smp_idx, ds_name, ds_label = data\n\n# X = Pipeline([('to_abs', ToAbsorbance()),\n#               ('snv', SNV())]).fit_transform(X)\n\n# X = Pipeline([('snv', SNV())]).fit_transform(X)\n\nX = Pipeline([\n    ('interpolate', Interpolate(X_names, wavenumbers_spik)),\n    ('snv', SNV())\n]).fit_transform(X)\n\nX_names = wavenumbers_spik\n\ny = Log1p().fit_transform(y)\n\nReading & selecting data ...\nCPU times: user 27.2 s, sys: 4.82 s, total: 32 s\nWall time: 34 s\n\n\n\nplot_spectra(X, X_names, alpha=0.5, ylabel='Absorbance', ascending=False)\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nfor i, ds in enumerate(ds_label): print(f'{ds_label[i]}: {len(X[ds_name == i])}')\n\nGARRETT.SSL: 183\nICRAF.ISRIC: 3672\nKSSL.SSL: 53230\nLUCAS.WOODWELL.SSL: 589\n\n\n\n# Train/valid split\nn_smp = None  # For demo. purpose\nX_train, X_valid, y_train, y_valid = train_test_split(X[:n_smp, :], y[:n_smp],\n                                                      test_size=0.1,\n                                                      stratify=ds_name[:n_smp],\n                                                      random_state=41)\n\n# Get PyTorch datasets\ntrain_ds, valid_ds = [SpectralDataset(X, y, )\n                      for X, y, in [(X_train, y_train), (X_valid, y_valid)]]\n\n\nX_train.shape\n\n(51906, 1738)\n\n\n\n# Then PyTorch dataloaders\ndls = get_dls(train_ds, valid_ds, bs=64)"
  },
  {
    "objectID": "paper/mir-kex.html#training",
    "href": "paper/mir-kex.html#training",
    "title": "MIR LSSM for Kex",
    "section": "Training",
    "text": "Training\n\ndef set_grad(m, b):\n    if isinstance(m, (nn.Linear,nn.BatchNorm2d)): return\n    if hasattr(m, 'weight'):\n        for p in m.parameters(): p.requires_grad_(b)\n\n\ndef get_n_params(model, trainable=True):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad == trainable)\n\n\nLearning from scratch\n\nmodel_name = 'resnet18'\nmodel = timm.create_model(model_name, pretrained=False,\n                          in_chans=1, num_classes=1)\n\n\n# Define modelling pipeline & Train\nepochs = 10\nlr = 5e-4\n\n# Define metrics\nmetrics = MetricsCB(r2=R2Score())\n\n# Define scheduler (how to vary learning rate during training)\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n\n# Callbacks\nxtra = [BatchSchedCB(sched)]\ngadf = BatchTransformCB(GADFTfm())\nresize = BatchTransformCB(_resizeTfm)\nstats = BatchTransformCB(StatsTfm(model.default_cfg))\n\ncbs = [DeviceCB(), gadf, resize, stats, TrainCB(),\n       metrics, ProgressCB(plot=False)]\n\nlearn = Learner(model, dls, nn.HuberLoss(), lr=lr,\n                cbs=cbs+xtra, opt_func=optim.AdamW)\n\n\nget_n_params(learn.model, trainable=True)\n\n11170753\n\n\n\nlearn.fit(epochs)\n\n\n\nNaive fine-tuning\n\nmodel_name = 'resnet18'\nmodel = timm.create_model(model_name, pretrained=True,\n                          in_chans=1, num_classes=1)\n\n# Define modelling pipeline & Train\nepochs = 5\nlr = 5e-4\n\n# Define metrics\nmetrics = MetricsCB(r2=R2Score())\n\n# Define scheduler (how to vary learning rate during training)\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n\n# Callbacks\nxtra = [BatchSchedCB(sched)]\ngadf = BatchTransformCB(GADFTfm())\nresize = BatchTransformCB(_resizeTfm)\nstats = BatchTransformCB(StatsTfm(model.default_cfg))\n\ncbs = [DeviceCB(), gadf, resize, stats, TrainCB(),\n       metrics, ProgressCB(plot=False)]\n\nlearn = Learner(model, dls, nn.HuberLoss(), lr=lr,\n                cbs=cbs+xtra, opt_func=optim.AdamW)\n\n\nget_n_params(learn.model, trainable=True)\n\n11170753\n\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nr2\nloss\nepoch\ntrain\n\n\n\n\n0.455\n0.038\n0\ntrain\n\n\n0.439\n0.040\n0\neval\n\n\n0.652\n0.025\n1\ntrain\n\n\n0.551\n0.033\n1\neval\n\n\n0.734\n0.019\n2\ntrain\n\n\n0.531\n0.034\n2\neval\n\n\n0.804\n0.014\n3\ntrain\n\n\n0.790\n0.015\n3\neval\n\n\n0.860\n0.010\n4\ntrain\n\n\n0.804\n0.014\n4\neval\n\n\n\n\n\nCPU times: user 8min 12s, sys: 2.64 s, total: 8min 15s\nWall time: 8min 12s\n\n\n\ntorch.save(learn.model.state_dict(), 'resnet-pretrained-02072024-mir.pth')\n\n\nPredicting\n\nmodel_name = 'resnet18'\nmodel = timm.create_model(model_name, pretrained=True,\n                          in_chans=1, num_classes=1)\n\nfname_model = \"../models/resnet-pretrained-03072024-mir-k-spiking.pth\"\nmodel.load_state_dict(torch.load(fname_model, map_location=torch.device('cpu')))\n\n# Define modelling pipeline & Train\nepochs = 1\nlr = 5e-4\n\n# Define metrics\nmetrics = MetricsCB(r2=R2Score())\n\n# Define scheduler (how to vary learning rate during training)\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n\n# Callbacks\nxtra = [BatchSchedCB(sched)]\ngadf = BatchTransformCB(GADFTfm())\nresize = BatchTransformCB(_resizeTfm)\nstats = BatchTransformCB(StatsTfm(model.default_cfg))\n\ncbs = [DeviceCB(device='cpu'), gadf, resize, stats, TrainCB(),\n       metrics]\n\nlearn = Learner(model, dls, nn.HuberLoss(), lr=lr,\n                cbs=cbs+xtra, opt_func=optim.AdamW)\n\n\nlearn.fit(epochs)\n\n\n# learn.model = model\n\n\n# X_mir_spik, wavenumbers_spik, names_spik = fc.load_pickle('../../_data/mir-k-spiking.pkl')\n\n\nnames_spik\n\narray(['LUI-0-0', 'LUI-0-1', 'LUI-0-2', 'LUI-0-3', 'LUI-0-4', 'LUI-1-0',\n       'LUI-1-1', 'LUI-1-2', 'LUI-1-3', 'LUI-1-4', 'LUI-2-0', 'LUI-2-1',\n       'LUI-2-2', 'LUI-2-3', 'LUI-2-4', 'LUI-3-0', 'LUI-3-1', 'LUI-3-2',\n       'LUI-3-3', 'LUI-3-4', 'SPA1-0-0', 'SPA1-0-2', 'SPA1-0-3',\n       'SPA1-0-4', 'SPA1-1-0', 'SPA1-1-1', 'SPA1-1-2', 'SPA1-1-3',\n       'SPA1-1-4', 'SPA1-2-0', 'SPA1-2-1', 'SPA1-2-2', 'SPA1-2-3',\n       'SPA1-2-4', 'SPA1-3-0', 'SPA1-3-1', 'SPA1-3-2', 'SPA1-3-3',\n       'SPA1-3-4', 'TM4.1-0-0', 'TM4.1-0-1', 'TM4.1-0-2', 'TM4.1-0-3',\n       'TM4.1-0-4', 'TM4.1-0-5', 'TM4.1-1-0', 'TM4.1-1-1', 'TM4.1-1-2',\n       'TM4.1-1-3', 'TM4.1-2-0', 'TM4.1-2-1', 'TM4.1-2-2', 'TM4.1-2-3',\n       'TM4.1-3-0', 'TM4.1-3-1', 'TM4.1-3-2', 'TM4.1-3-3', 'TM4.1-3-4'],\n      dtype='&lt;U9')\n\n\n\nnp.mean(np.expm1(y))\n\n0.6474079249216373\n\n\n\ndef predict(X, substring, names, learn):\n    mask = [substring in name for name in names_spik]\n    return learn.get_preds(X[mask], y_tfm_fn=np.expm1)\n\nfor smp_name in ['LUI', 'SPA1', 'TM4.1']:\n    print(f'Sample name: {smp_name}')\n    for substring in [f'{smp_name}-{i}' for i in range(4)]:\n        preds = predict(X_mir_spik, substring, names_spik, learn)\n        print(f'Spiking level: {substring}, mean: {np.mean(preds):.3}, std: {np.std(preds):.3}')\n    print(80*'-')\n\nSample name: LUI\nSpiking level: LUI-0, mean: 0.238, std: 0.02\nSpiking level: LUI-1, mean: 0.266, std: 0.0293\nSpiking level: LUI-2, mean: 0.179, std: 0.0602\nSpiking level: LUI-3, mean: 0.232, std: 0.0399\n--------------------------------------------------------------------------------\nSample name: SPA1\nSpiking level: SPA1-0, mean: 2.26, std: 0.459\nSpiking level: SPA1-1, mean: 2.29, std: 0.249\nSpiking level: SPA1-2, mean: 3.16, std: 0.974\nSpiking level: SPA1-3, mean: 2.88, std: 0.38\n--------------------------------------------------------------------------------\nSample name: TM4.1\nSpiking level: TM4.1-0, mean: 0.702, std: 0.297\nSpiking level: TM4.1-1, mean: 0.329, std: 0.22\nSpiking level: TM4.1-2, mean: 0.667, std: 0.0986\nSpiking level: TM4.1-3, mean: 0.409, std: 0.057\n--------------------------------------------------------------------------------\n\n\n\n\n\nFreezing batch norm & linear layers\n\nmodel_name = 'resnet18'\nmodel = timm.create_model(model_name, pretrained=True,\n                          in_chans=1, num_classes=1)\n\n# Define modelling pipeline & Train\nepochs = 3\nlr = 5e-4\n\n# Define metrics\nmetrics = MetricsCB(r2=R2Score())\n\n# Define scheduler (how to vary learning rate during training)\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n\n# Callbacks\nxtra = [BatchSchedCB(sched)]\ngadf = BatchTransformCB(GADFTfm())\nresize = BatchTransformCB(_resizeTfm)\nstats = BatchTransformCB(StatsTfm(model.default_cfg))\n\ncbs = [DeviceCB(), gadf, resize, stats, TrainCB(),\n       metrics, ProgressCB(plot=False)]\n\nlearn = Learner(model, dls, nn.HuberLoss(), lr=lr,\n                cbs=cbs+xtra, opt_func=optim.AdamW)\n\n\nlearn.model.apply(partial(set_grad, b=False));\n\n\nget_n_params(learn.model, trainable=True)\n\n10113\n\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nr2\nloss\nepoch\ntrain\n\n\n\n\n0.154\n0.058\n0\ntrain\n\n\n0.433\n0.041\n0\neval\n\n\n0.473\n0.037\n1\ntrain\n\n\n0.507\n0.035\n1\neval\n\n\n0.526\n0.033\n2\ntrain\n\n\n0.535\n0.033\n2\neval\n\n\n\n\n\nCPU times: user 3min 40s, sys: 4.66 s, total: 3min 44s\nWall time: 3min 39s\n\n\n\n# Now unfreeze all\nlearn.model.apply(partial(set_grad, b=True));\n\n\nget_n_params(learn.model, trainable=True)\n\n11170753\n\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nr2\nloss\nepoch\ntrain\n\n\n\n\n0.578\n0.030\n0\ntrain\n\n\n0.201\n0.057\n0\neval\n\n\n0.698\n0.021\n1\ntrain\n\n\n0.731\n0.020\n1\neval\n\n\n0.802\n0.014\n2\ntrain\n\n\n0.780\n0.016\n2\neval\n\n\n\n\n\nCPU times: user 4min 43s, sys: 5.23 s, total: 4min 48s\nWall time: 4min 42s\n\n\n\n\nLearning rate finder\n\nmodel_name = 'resnet18'\nmodel = timm.create_model(model_name, pretrained=False,\n                          in_chans=1, num_classes=1)\n\n\n# Define modelling pipeline & Train\nepochs = 3\nlr = 5e-3\n\nmetrics = MetricsCB(r2=R2Score())\n\ntmax = epochs * len(dls.train)\n\ngadf = BatchTransformCB(GADFTfm())\nresize = BatchTransformCB(_resizeTfm)\nstats = BatchTransformCB(StatsTfm(model.default_cfg))\n\ncbs = [DeviceCB(), gadf, resize, stats, TrainCB(),\n       metrics, ProgressCB(plot=False)]\n\nlearn = Learner(model, dls, nn.HuberLoss(), lr=lr,\n                cbs=cbs, opt_func=optim.AdamW)\n\n\n# gamma=1.3, max_mult=3, start_lr=1e-5, max_epochs=10\nlearn.lr_find(gamma=1.1,start_lr=1e-5, max_mult=10)\n\n\n\n\n\n\n    \n      \n      0.00% [0/10 00:00&lt;?]\n    \n    \n\n\n    \n      \n      9.98% [81/812 00:09&lt;01:21 0.125]"
  },
  {
    "objectID": "hooks.html",
    "href": "hooks.html",
    "title": "Hooks",
    "section": "",
    "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nanalytes = 'clay.tot_usda.a334_w.pct' # Let's play with Clay\nspectra_type = 'mir'\n\ndata = load_ossl(analytes, spectra_type)\nX, y, X_names, smp_idx, ds_name, ds_label = data\n\nX = Pipeline([('snv', SNV())]).fit_transform(X)\ny = Log1p().fit_transform(y)\n\nReading & selecting data ...\n\n\n\n# Train/valid split\nn_smp = None # For demo. purpose\nX_train, X_valid, y_train, y_valid = train_test_split(X[:n_smp, :], y[:n_smp],\n                                                      test_size=0.1,\n                                                      stratify=ds_name[:n_smp],\n                                                      random_state=41)\n\n# Get PyTorch datasets\ntrain_ds, valid_ds = [SpectralDataset(X, y, )\n                      for X, y, in [(X_train, y_train), (X_valid, y_valid)]]\n\n# Then PyTorch dataloaders\ndls = get_dls(train_ds, valid_ds, bs=32)\n\n\nsource\n\nToyCNN\n\n ToyCNN ()\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nepochs = 8\nlr = 1e-3\n\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched)]\ncbs = [DeviceCB(), TrainCB(), \n       MetricsCB(r2=R2Score()),\n       ProgressCB(plot=False)]\n\nlearn = Learner(ToyCNN(), dls, nn.MSELoss(), lr=lr,\n                cbs=cbs+xtra, opt_func=optim.AdamW)\n\n\nlearn.fit(epochs)\n\nr2        loss      epoch     train                                                         \n0.002     0.809     0         train     \n0.739     0.213     0         eval                                                      \n0.793     0.168     1         train                                                         \n0.746     0.207     1         eval                                                      \n0.825     0.142     2         train                                                         \n0.816     0.150     2         eval                                                      \n0.844     0.127     3         train                                                         \n0.851     0.121     3         eval                                                      \n0.855     0.117     4         train                                                         \n0.856     0.117     4         eval                                                      \n0.866     0.109     5         train                                                         \n0.864     0.111     5         eval                                                      \n0.873     0.103     6         train                                                         \n0.868     0.107     6         eval                                                      \n0.878     0.099     7         train                                                         \n0.870     0.106     7         eval                                                      \n\n\n\nsource\n\n\nHook\n\n Hook (m, f)\n\nInitialize self. See help(type(self)) for accurate signature.\n\ndef fit(model, epochs=1, xtra_cbs=None):\n    learn = Learner(model, dls, loss_func=F.cross_entropy, lr=0.6, cbs=cbs+fc.L(xtra_cbs))\n    learn.fit(epochs)\n    return learn\n\n\ndef append_stats(hook, mod, inp, outp):\n    if not hasattr(hook,'stats'): hook.stats = ([],[])\n    acts = to_cpu(outp)\n    hook.stats[0].append(acts.mean())\n    hook.stats[1].append(acts.std())\n\n\nimport nbdev; nbdev.nbdev_export()",
    "crumbs": [
      "API",
      "Hooks"
    ]
  }
]