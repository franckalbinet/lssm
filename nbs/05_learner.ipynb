{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner\n",
    "\n",
    "> Generic learner & utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Generic and highly flexible learner given the ability to inject behaviour almost every in the pipeline using callbacks. For further details and to give credit where credit is due, please refers to: [FastAI course part II](https://course.fast.ai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from functools import partial\n",
    "\n",
    "import fastcore.all as fc\n",
    "from collections.abc import Mapping\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as T\n",
    "from lssm.callbacks import (run_cbs, to_cpu, LRFinderCB,\n",
    "                            CancelBatchException, CancelFitException, CancelEpochException)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import timm\n",
    "from torcheval.metrics import R2Score\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from lssm.loading import load_ossl\n",
    "from lssm.preprocessing import ToAbsorbance, ContinuumRemoval, Log1p\n",
    "from lssm.dataloaders import SpectralDataset, get_dls\n",
    "from lssm.callbacks import (MetricsCB, BatchSchedCB, BatchTransformCB,\n",
    "                            DeviceCB, TrainCB, ProgressCB)\n",
    "from lssm.transforms import GADFTfm, _resizeTfm, StatsTfm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class with_cbs:\n",
    "    'Decorator calling \"before_`nm`\" and \"after_`nm`\" around the decorated method. Call \"cleanup_`nm`\" once done.'\n",
    "    def __init__(self, \n",
    "                 nm:str # Name of the callback method to call\n",
    "                 ): \n",
    "        self.nm = nm\n",
    "    def __call__(self, f):\n",
    "        def _f(o, *args, **kwargs):\n",
    "            try:\n",
    "                o.callback(f'before_{self.nm}')\n",
    "                f(o, *args, **kwargs)\n",
    "                o.callback(f'after_{self.nm}')\n",
    "            except globals()[f'Cancel{self.nm.title()}Exception']: pass\n",
    "            finally: o.callback(f'cleanup_{self.nm}')\n",
    "        return _f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Learner():\n",
    "    def __init__(self,\n",
    "                 model, \n",
    "                 dls=(0,),\n",
    "                 loss_func=F.mse_loss,\n",
    "                 lr=0.1,\n",
    "                 cbs=None,\n",
    "                 opt_func=optim.SGD\n",
    "                 ):\n",
    "        cbs = fc.L(cbs)\n",
    "        fc.store_attr()\n",
    "\n",
    "    @with_cbs('batch')\n",
    "    def _one_batch(self):\n",
    "        self.predict()\n",
    "        self.callback('after_predict')\n",
    "        self.get_loss()\n",
    "        self.callback('after_loss')\n",
    "        if self.training:\n",
    "            self.backward()\n",
    "            self.callback('after_backward')\n",
    "            self.step()\n",
    "            self.callback('after_step')\n",
    "            self.zero_grad()\n",
    "\n",
    "    @with_cbs('epoch')\n",
    "    def _one_epoch(self):\n",
    "        for self.iter,self.batch in enumerate(self.dl):\n",
    "            self._one_batch()\n",
    "\n",
    "    def one_epoch(self, training):\n",
    "        self.model.train(training)\n",
    "        self.dl = self.dls.train if training else self.dls.valid\n",
    "        self._one_epoch()\n",
    "\n",
    "    @with_cbs('fit')\n",
    "    def _fit(self, train, valid):\n",
    "        for self.epoch in self.epochs:\n",
    "            if train: self.one_epoch(True)\n",
    "            if valid: torch.no_grad()(self.one_epoch)(False)\n",
    "\n",
    "    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n",
    "        cbs = fc.L(cbs)\n",
    "        # `add_cb` and `rm_cb` were added in lesson 18\n",
    "        for cb in cbs: self.cbs.append(cb)\n",
    "        try:\n",
    "            self.n_epochs = n_epochs\n",
    "            self.epochs = range(n_epochs)\n",
    "            if lr is None: lr = self.lr\n",
    "            if self.opt_func: self.opt = self.opt_func(self.model.parameters(), lr)\n",
    "            self._fit(train, valid)\n",
    "        finally:\n",
    "            for cb in cbs: self.cbs.remove(cb)\n",
    "\n",
    "    def get_preds(self, data):\n",
    "        self.model.train(False)\n",
    "        self.batch = data\n",
    "        self._one_batch()\n",
    "        return learn.preds\n",
    "        \n",
    "    def __getattr__(self, name):\n",
    "        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)\n",
    "        raise AttributeError(name)\n",
    "\n",
    "    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n",
    "    \n",
    "    @property\n",
    "    def training(self): return self.model.training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44489/44489 [00:15<00:00, 2859.72it/s]\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "model_name = 'resnet18'\n",
    "model = timm.create_model(model_name, pretrained=True, in_chans=1, num_classes=1)\n",
    "\n",
    "# Load dataset\n",
    "analytes = 'k.ext_usda.a725_cmolc.kg'\n",
    "data = load_ossl(analytes, spectra_type='visnir')\n",
    "X, y, X_names, smp_idx, ds_name, ds_label = data\n",
    "\n",
    "# Preprocess\n",
    "X = Pipeline([('to_abs', ToAbsorbance()), \n",
    "              ('cr', ContinuumRemoval(X_names))]).fit_transform(X)\n",
    "y = Log1p().fit_transform(y)\n",
    "\n",
    "# Train/valid split\n",
    "n_smp = None # For demo. purpose\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X[:n_smp, :], y[:n_smp], \n",
    "                                                      test_size=0.1,\n",
    "                                                      stratify=ds_name[:n_smp], \n",
    "                                                      random_state=41)\n",
    "\n",
    "# Get PyTorch datasets\n",
    "train_ds, valid_ds = [SpectralDataset(X, y, ) \n",
    "                      for X, y, in [(X_train, y_train), (X_valid, y_valid)]]\n",
    "\n",
    "# Then PyTorch dataloaders\n",
    "dls = get_dls(train_ds, valid_ds, bs=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>r2</th>\n",
       "      <th>loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.360</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.539</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|eval: false\n",
    "\n",
    "# Define modelling pipeline & Train\n",
    "epochs = 1\n",
    "lr = 5e-3\n",
    "\n",
    "metrics = MetricsCB(r2=R2Score())\n",
    "\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "\n",
    "xtra = [BatchSchedCB(sched)]\n",
    "\n",
    "gadf = BatchTransformCB(GADFTfm())\n",
    "resize = BatchTransformCB(_resizeTfm)\n",
    "stats = BatchTransformCB(StatsTfm(model.default_cfg))\n",
    "\n",
    "cbs = [DeviceCB(), gadf, resize, stats, TrainCB(), \n",
    "       metrics, ProgressCB(plot=False)]\n",
    "\n",
    "learn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "\n",
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@fc.patch\n",
    "def lr_find(self:Learner, gamma=1.3, max_mult=3, start_lr=1e-5, max_epochs=10):\n",
    "    self.fit(max_epochs, lr=start_lr, cbs=LRFinderCB(gamma=gamma, max_mult=max_mult))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m = nn.Conv1d(16, 33, 3, stride=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
